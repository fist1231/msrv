docker:
=======
##########################################  Docker   ###########################################################

$ docker network ls
$ docker run --network=<NETWORK>
$ docker network inspect bridge

$ docker network create -d bridge --subnet 192.168.0.0/24 --gateway 192.168.0.1 dockernet
$ docker network rm dockernet

$ docker network create -d bridge --subnet 172.17.0.0/24 --gateway 172.17.0.1 dockernet

$ docker attach container1

docker info

You can share a drive “on demand” the first time a particular mount is requested:

docker --rm -v c:/Users:/data ls /data run hello-world
docker run hello-world

docker run  -it ubuntu bash | exit

docker run -d -p 80:80 --name webserver nginx
192.168.99.100
docker ps
docker stop webserver
docker start webserver
docker rmi nginx

~~~~~~~~~~~~~~~~~~~~~~

Windows Power Shell run as admin
Set-ExecutionPolicy RemoteSigned
Install-Module posh-docker
Import-Module posh-docker

if (-Not (Test-Path $PROFILE)) {
    New-Item $PROFILE –Type File –Force
}

Add-Content $PROFILE "`nImport-Module posh-docker"

This creates a $PROFILE if one does not already exist, and adds this line into the file:
Import-Module posh-docker
To check that the file was properly created, or simply edit it manually, type this in PowerShell:
Notepad $PROFILE

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



docker run --rm -v c:/Users:/data ls /data -i -t -p 3333:3333 --network="host" users-node:v1 /bin/bash

===============
Local Registry:
===============

docker run -d -p 5000:5000 --restart=always --name registry registry:2

docker pull ubuntu:16.04
docker tag ubuntu:16.04 localhost:5000/my-ubuntu

Push the image to the local registry running at localhost:5000:
docker push localhost:5000/my-ubuntu

Remove the locally-cached ubuntu:16.04 and localhost:5000/my-ubuntu images, so that you can test pulling the image from your registry. This does not remove the localhost:5000/my-ubuntu image from your registry.

docker image remove ubuntu:16.04
docker image remove localhost:5000/my-ubuntu

Pull the localhost:5000/my-ubuntu image from your local registry.

======================
Stop a local registry:
======================

docker stop registry

To remove the container, use docker rm
docker stop registry && docker rm -v registry
docker rmi -f registry
docker rmi -f registry:2


-create server.js
-create Dockerfile
docker build -t n1-node:v1 .


docker images
docker ps
192.168.99.100

docker build -t hello-node:v1 .

docker run -i -t -p 8080:8080 hello-node:v1 /bin/bash

2. another node
docker build -t bcx-node:v1 .
docker run -i -t -p 5555:5555 bcx-node:v1 /bin/bash


docker registry in docker:
=========================

docker run -d -p 5000:5000 --name registry registry:2
docker pull ubuntu
docker tag ubuntu localhost:5000/myfirstimage
docker push localhost:5000/myfirstimage
docker pull localhost:5000/myfirstimage
docker stop registry && docker rm -v registry


docker tag ubuntu localhost:5000/n1-node:v1
docker push localhost:5000/n1-node:v1
docker pull localhost:5000/n1-node:v1



##########################################  Minikube   ###########################################################

minikube:
========

start minikube:
==============
minikube status

minikube stop
minikube start --vm-driver="virtualbox"

minikube start --vm-driver="virtualbox" --insecure-registry="0.0.0.0:5000"
kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100

create deployment:

kubectl delete deployment n1-node
kubectl run n1-node --image=n1-node:v1 --port=8080




kubectl get pod
kubectl get deployments

create service:

-expose
kubectl expose deployment n1-node --type=LoadBalancer
kubectl get services

-start service:
minikube service n1-node

http://192.168.99.101:31528/


CLUSTER IP:
===========
kubectl run n1-5-node --replicas=5 --labels="run=load-balancer-example" --image=n1-node:v1  --port=8080
-kubectl get deployments n1-5-node
-kubectl describe deployments n1-5-node
-kubectl get replicasets
-kubectl describe replicasets

//kubectl expose deployment n1-5-node --type=LoadBalancer --name=n1-5-service    not working on single minikube
kubectl expose deployment n1-5-node --type=NodePort --name=n1-5-service
-kubectl get services n1-5-service
-kubectl describe services n1-5-service

minikube service n1-5-service

kubectl delete services n1-5-service
kubectl delete deployment n1-5-node


==============================================================================================================================

Docker image users.js:
======================
1. docker quickstart shortcut
2. cd c:/angular/msrv/services/nress/users
3. docker build -t users-node:v1 .
4. docker images
docker ps

5. Test: 
# docker run -i -t -p 3333:3333 users-node:v1 /bin/bash
docker run -i -t -p 3333:3333 --network=host users-node:v1 /bin/bash

remove image
docker rmi -f users-node:v1


$ ip addr show eth0


dockerfile from:
===============
FROM node:8.9.1
EXPOSE 3333
COPY ./ .
CMD node users.js

to:
===


RUN npm install -g nodemon
COPY ./script.js /root/script.js
CMD ["nodemon", "-w", "./users.js", "./users.js"]


Move to registry (drop/re-create registry first):
================================================

#docker run -v $(pwd)/data:/tmp/registry-dev --name docker-registry registry:2.0

#docker run -p 443:443 -e REGISTRY_HOST="docker-registry" -e REGISTRY_PORT="5000" -e SERVER_NAME="localhost" --link docker-registry:docker-registry 


docker stop registry && docker rm -v registry
docker run -d -p 5000:5000 --name registry registry:2

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
~~~~~~~~~~~~~~~~~~~~~
Build from Dokerfile:
~~~~~~~~~~~~~~~~~~~~~
1. Build:
$ docker build -t users-node:v1 .
$ docker images

2. Test:
$ docker run -i -t -p 3333:3333 --network=host users-node:v1 /bin/bash
# nodemon -w ./users.js ./users.js
http://192.168.99.100:3333/nress/users

2. Create Registry, if not present (delete old ones):

docker stop registry && docker rm -v registry
docker rmi -f registry
docker rmi -f registry:2

// $ docker run -d -p 5000:5000 --restart=always --name registry registry:2 - https
// $ docker run -d -p 5000:5000 --restart=always --name registry registry
$ docker run -d -p 5000:5000 --network=host --restart=always --name registry registry:2

3. Put to local registry for Kubernetes:
// $ docker tag users-node:v1 localhost:5000/users-node
// $ docker push localhost:5000/users-node
$ docker tag users-node:v1 192.168.99.100:5000/users-node
$ docker push 192.168.99.100:5000/users-node

-Remove old images:
$ docker image remove users-node:v1
or
$ docker rmi -f users-node:v1

$ docker image remove localhost:5000/users-node

-Test pull now from local registry:
// $ docker pull localhost:5000/users-node
$ docker pull 192.168.99.100:5000/users-node

- Test again:
$ docker run -i -t -p 3333:3333 --network=host localhost:5000/users-node /bin/bash
# nodemon -w ./users.js ./users.js
http://192.168.99.100:3333/nress/users

Fin.

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
To kubernetes: (start power shell with admin priv)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Exit Docker, if present (shortcut kill). Minikube will start its own.

2. Start minikube (192.168.99.106)
minikube delete

minikube start --insecure-registry "10.0.0.0/24"
#minikube start --vm-driver="virtualbox" - no
#minikube start --vm-driver="virtualbox" --insecure-registry="localhost:5000" - no

minikube start --vm-driver="virtualbox" --insecure-registry="0.0.0.0:5000"
#minikube start --vm-driver="virtualbox" --insecure-registry="192.168.99.100:5000"  - connection refused

~/.minikube/machines/minikube/config.json

minikube status
#kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.103
#kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.101

kubectl get pod
kubectl get deployments
kubectl delete deployment users-node

#kubectl create secret docker-registry my-secret --docker-server=127.0.0.1:5000
# kubectl create secret docker-registry my-secret --docker-server=127.0.0.1:5000 --docker-username=uname --docker-password=****** --docker-email=email@sample.com

kubectl run users-node --image=localhost:5000/users-node --port=3333
kubectl get pod

kubectl run users-node --replicas=5 --labels="users-node minikube service" --image=localhost:5000/users-node --port=3333

2. kubectl expose deployment users-node --type=NodePort --name=users-service
kubectl delete services/users-service
kubectl delete deployment users-node

3. minikube service users-service

-----------------------------------------------------------------------------------------------------------------------------------------------------------
For Mongo ip access:

ipconfig /all
Ethernet adapter VirtualBox Host-Only Network: IPv4 Address

kubectl get po --all-namespaces
//if no dns module, do these:

minikube stop
PS C:\Program Files\Oracle\VirtualBox> 
.\VBoxManage.exe modifyvm "minikube" --natdnsproxy1 on
.\VBoxManage.exe modifyvm "minikube" --natdnshostresolver1 on

minikube start


minikube start --vm-driver="virtualbox"
minikube docker-env
minikube docker-env | Invoke-Expression
docker images
...

docker build -t users-node:v1 .
kubectl run users-node --replicas=5 --image=users-node:v1 --port=3333

sudo kubectl run users-node --replicas=5 --image=users-node:v1 --port=3333 --env="DOMAIN=cluster"

kubectl get pod

or for own port goto 2:

kubectl expose  users-node --type=NodePort --name=users-service
// kubectl expose deployment users-node --type=LoadBalancer --name=users-service
kubectl get service

users-service   NodePort    10.103.136.109   <none>        3333:31367/TCP   12s

http://192.168.99.103:31367/nress/users

kubectl get namespaces
$ curl $(minikube service users-node --url)

kubectl describe services users-service
kubectl cluster-info

2:
==
get yaml:
kubectl edit svc users-service
add nodePort: 31234  (range: The range of valid ports is 30000-32767)

save as users-service.yaml

kubectl delete service users-service

kubectl create -f users-service.yaml




##########################################  Kubernetes Cluster   ###########################################################

https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network


====================== Ubuntu 17.04 kubernetes ==================================
1. Install dependency library
sudo apt-get update 
sudo apt-get install -y apt-transport-https

2. Docker:
sudo apt install docker.io

Docker is enabled upon boot:
sudo systemctl enable docker.service

Once that completes, start and enable the Docker service with the commands

sudo systemctl start docker
sudo systemctl enable docker

3. Install Kubernetes:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
3.1: For nodes - clone machine.
Didn't work, created new vm

Change hostname:
sudo nano /etc/hostname
sudo nano /etc/hosts

systemctl restart systemd-logind.service
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

//vi /etc/resolv.conf
//nameserver 8.8.8.8
//nameserver 8.8.4.4


sudo apt-get install resolvconf
/etc/resolvconf/resolv.conf.d/tail
nameserver 8.8.8.8
nameserver 8.8.4.4
sudo resolvconf -u
will add to /etc/resolv.conf

- Add key for new repository and add repository
sudo curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add 

- creating the file /etc/apt/sources.list.d/kubernetes.list and enter the following content:
deb http://apt.kubernetes.io/ kubernetes-xenial main
Save and close that file. Install Kubernetes with the following commands:

- install kubernetes
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl kubernetes-cni

Comment out your swap lines in /etc/fstab

4. Initialize your master
With everything installed, go to the machine that will serve as the Kubernetes master and issue the command:

(sudo kubeadm reset)
sudo swapoff -a
sudo kubeadm init

or

Didn't work:
# echo 'Environment="KUBELET_EXTRA_ARGS=--fail-swap-on=false"' > /etc/systemd/system/kubelet.service.d/90-local-extras.conf
# systemctl daemon-reload
# systemctl restart kubelet

/healthz: dial tcp 127.0.0.1:10255: getsockopt: connection refused.

sudo ufw status

Didn't work: 
// sudo nano /etc/default/ufw
// DEFAULT_FORWARD_POLICY="DROP"
// to
// DEFAULT_FORWARD_POLICY="ACCEPT"
// sudo ufw reload

Didn't work:
// sudo ufw disable
// sudo ufw enable

sudo kubeadm reset
add "Environment="KUBELET_EXTRA_ARGS=--fail-swap-on=false"" to /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
sudo systemctl daemon-reload
sudo systemctl restart kubelet
//sudo kubeadm init --skip-preflight-checks

// sudo kubeadm init --node-name master - Not working, hangs after 'Will mark node master as master by adding a label and a taint'

// If usinf with flannel (https://coreos.com/flannel/docs/latest/kubernetes.html):
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Node1:

sudo kubeadm join --token 565014.35b9f79d771c273d

kubectl get nodes

//////////////////////////////////////////////////////////////////////////////////

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token 48fc6f.fd4313ca8539efc7 192.168.1.208:6443 --discovery-token-ca-cert-hash sha256:4b4027ed823e14f7adf46ec57fd1f731d0485354ee51735ddacb75e319106f6a
/////////////////////////////////////////////////////////////////////////////////

kubectl get node


kubectl -n kube-system get secret clusterinfo -o yaml | grep token-map | awk '{print $2}' | base64 --decode | sed "s|{||g;s|}||g;s|:|.|g;s/\"//g;" | xargs echo
sudo kubeadm token list

For the master:
==============
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

For the nodes:
=============
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/kubelet.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


Not working.
//6. Deploy weave CNI:
//kubectl apply -f https://git.io/weave-kube

6. Deploying a pod network, Flannel

- kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
  kubectl apply -f hhttps://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml
  kubectl apply -f https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-aliyun.yml

kubectl apply -f https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml

7. Enable master node run pod [optional]
kubectl taint nodes --all node-role.kubernetes.io/master-

sudo kubectl get pods
kubectl cluster-info
https://192.168.1.208:6443/api/v1/namespaces/kube-system/services/kube-dns/proxy

kubectl get pods --namespace=kube-system
kubectl get pods --all-namespaces

~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Setup node1:
1. Setup machine same as master up to 4. Initialize your master exclusive.
2. Run command kubeadm join with params is the secret key of your kubernetes cluser and your master node ip:

kubeadm join --token 858698.51d1418b0490485a 192.168.0.13

kubeadm join --token 1da045.04ba46425fcb7129 10.0.2.15:6443 --discovery-token-ca-cert-hash sha256:cb4448ba840862b141d9c51bc3bd094fd6829eb30ee412addbe2b6673f96c3b1


************************************************************************************
Oracle VirtualBox 5.2.2 setup windows10, 2 machines with ubuntu 17.10, access each other and internet on  both:

1. File-proferences-network-add; enable dhcp, edit dhcp: 10.0.2.0/24
2. Each VM, network. 
- Adapter1: Nat Network - NatNetwork name, Promisc node: Allow all. 
- Adapter2: Host Only, VirtualBox Host-Only adapter: Promisc node: Allow all; DHCP in global tools:
	Adapter tab: ipv4-10.0.2.1; mask 255.255.255.0
	DHCP tab: 10.0.2.1; 255.255.255.0; 10.0.2.2 - 10.0.2.254
	
	
**************************************************************************************	
Tear down of the node

To undo what kubeadm did, you should first drain the node and make sure that the node is empty before shutting it down.
Talking to the master with the appropriate credentials, run:
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>
Then, on the node being removed, reset all kubeadm installed state:
kubeadm reset
If you wish to start over simply run kubeadm init or kubeadm join with the appropriate arguments.
Note: kubeadm reset will not delete any etcd data if external etcd is used. This means that if you run kubeadm init again using the same etcd endpoints, you will see state from previous clusters. To wipe etcd data after reset, it is recommended you use a client like etcdctl, such as:
etcdctl del "" --prefix	
	
	

docker build -t users-node:v1 .	
sudo kubectl run users-node --replicas=5 --image=users-node:v1 --port=3333

===============================================================================================================================================================================

Certs for secure registry:

Docker:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
$ mkdir -p certs

$ openssl req \
  -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \
  -x509 -days 365 -out certs/domain.crt
Be sure to use the name ubt-master (master node machine name) as a CN.

Copy /certs/domain.crt to all nodes oon the cluster:
sudo scp ~/certs/domain.crt username@ubt-node1:/etc/docker/certs.d/ubt-master:443/domain.crt
create certs.d/ubt-master:443, if missing. No restart necessary

#Stop the registry if it is currently running.
$ docker stop registry

docker run -d \
  --restart=always \
  --name registry \
  -v `pwd`/certs:/certs \
  -e REGISTRY_HTTP_ADDR=0.0.0.0:443 \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \
  -p 443:443 \
  registry:2

sudo docker tag users-node:v1 ubt-master:443/users-node:v1
sudo docker push ubt-master:443/users-node:v1


To clear:
*********
sudo docker ps -a | less
sudo docker rm -f <registry id>

sudo docker images
sudo docker rmi -f <image_ids>
sudo docker rmi -f registry:2
******************************

sudo kubectl run users-node --replicas=5 --image=ubt-master:443/users-node:v1 --port=3333
sudo kubectl get pods

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
nginx:
kubectl create deployment nginx --image=nginx
kubectl create service nodeport nginx --tcp=80:80

kubectl expose deployment users-node --type=NodePort --name=users-service

kubectl get svc
curl kube-worker-1:32555




